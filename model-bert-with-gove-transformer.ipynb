{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# used this tutorial to help. \n# https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert\n# https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n# https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert#import_libraries\n\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import GlobalAveragePooling1D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import GlobalAveragePooling1D\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\nfrom transformers import BertTokenizer, AutoTokenizer, AutoModelForSequenceClassification, TFBertModel, BertConfig\n\nfrom tqdm import tqdm\n# plots and images\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\n\n#sklearn processing\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\nimport os\n\n# import wandb\n\n\n# #configs\n# max_tokens = 20000\n# embed_dim  = 300\n# num_heads  = 2\n# dense_dim  = 32\n# development_mode = True\n\n\nBATCH_SIZE = 12\nBUFFER_SIZE = 3200\nSEQ_LEN = 512\nEPOCHS = 25\nAUTO = tf.data.AUTOTUNE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-29T02:44:02.928681Z","iopub.execute_input":"2022-11-29T02:44:02.929303Z","iopub.status.idle":"2022-11-29T02:44:02.945244Z","shell.execute_reply.started":"2022-11-29T02:44:02.929246Z","shell.execute_reply":"2022-11-29T02:44:02.944185Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"/kaggle/input/bertbasecased/config.json\n/kaggle/input/bertbasecased/tokenizer.json\n/kaggle/input/bertbasecased/tokenizer_config.json\n/kaggle/input/bertbasecased/pytorch_model.bin\n/kaggle/input/bertbasecased/vocab.txt\n/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv\n/kaggle/input/feedback-prize-english-language-learning/train.csv\n/kaggle/input/feedback-prize-english-language-learning/test.csv\n/kaggle/input/glove/glove.6B.300d.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/train.csv')\ntest = pd.read_csv('/kaggle/input/feedback-prize-english-language-learning/test.csv')\n\n\ntargets=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n\ntrain_y = train[targets]\n\ntrain_x = train['full_text']\ntest_x = test['full_text']","metadata":{"execution":{"iopub.status.busy":"2022-11-29T02:02:56.368363Z","iopub.execute_input":"2022-11-29T02:02:56.368713Z","iopub.status.idle":"2022-11-29T02:02:56.455707Z","shell.execute_reply.started":"2022-11-29T02:02:56.368684Z","shell.execute_reply":"2022-11-29T02:02:56.454792Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# process glove embeddings.\npath_to_glove_file = '/kaggle/input/glove/glove.6B.300d.txt'\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep= \" \")\n        embeddings_index[word] = coefs\n\nprint(f'Found {len(embeddings_index)} word vectors')","metadata":{"execution":{"iopub.status.busy":"2022-11-29T01:58:36.488740Z","iopub.execute_input":"2022-11-29T01:58:36.489086Z","iopub.status.idle":"2022-11-29T01:58:59.395612Z","shell.execute_reply.started":"2022-11-29T01:58:36.489054Z","shell.execute_reply":"2022-11-29T01:58:59.394552Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found 400000 word vectors\n","output_type":"stream"}]},{"cell_type":"code","source":"max_tokens = 20000\nembed_dim  = 300\nnum_heads  = 2\ndense_dim  = 32\n\n\ntext_vectorization = layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=6000,\n)\n\n#use the dataset to index the dataset vocab via the adapt method\ntext_vectorization.adapt(train_x)\n\ntrain_full_x = text_vectorization(train_x)\ntest_full_x = text_vectorization(test_x)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T02:03:02.217750Z","iopub.execute_input":"2022-11-29T02:03:02.218102Z","iopub.status.idle":"2022-11-29T02:03:03.517825Z","shell.execute_reply.started":"2022-11-29T02:03:02.218071Z","shell.execute_reply":"2022-11-29T02:03:03.516829Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2022-11-29 02:03:02.289721: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"vocabulary = text_vectorization.get_vocabulary() #get vocab from text_vectorization function\nword_index = dict(zip(vocabulary, range(len(vocabulary)))) #use vocabulary to map from words to their index in the vocab\n\nembedding_matrix = np.zeros((max_tokens, embed_dim))\nfor word, i in word_index.items():\n    if i < max_tokens:\n        embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-11-29T02:05:38.661055Z","iopub.execute_input":"2022-11-29T02:05:38.661439Z","iopub.status.idle":"2022-11-29T02:05:38.759114Z","shell.execute_reply.started":"2022-11-29T02:05:38.661406Z","shell.execute_reply":"2022-11-29T02:05:38.758078Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"/kaggle/input/bertbasecased/\")\n\ndef get_ids_mask(inputs):\n    input_ids = []\n    attention_mask = []\n    for x in tqdm(inputs):\n        tokens = tokenizer(x, padding=\"max_length\", truncation=True, max_length=SEQ_LEN, return_tensors=\"np\")\n        ids = tokens[\"input_ids\"]\n        mask = tokens[\"attention_mask\"]\n        input_ids.append(ids)\n        attention_mask.append(mask)\n    input_ids = np.array(input_ids).squeeze()\n    attention_mask = np.array(attention_mask).squeeze()\n    return input_ids, attention_mask\n\n\ntrain_input_ids, train_attention_mask = get_ids_mask(train_x)\ntest_input_ids, test_attention_mask = get_ids_mask(test_x)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-29T02:12:34.962622Z","iopub.execute_input":"2022-11-29T02:12:34.962985Z","iopub.status.idle":"2022-11-29T02:13:11.991804Z","shell.execute_reply.started":"2022-11-29T02:12:34.962952Z","shell.execute_reply":"2022-11-29T02:13:11.990837Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 3911/3911 [00:36<00:00, 105.90it/s]\n100%|██████████| 3/3 [00:00<00:00, 90.03it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"\nclass MeanPool(keras.layers.Layer):\n    def call(self, x, mask=None):\n        broad_mask = tf.cast(tf.expand_dims(mask, -1), \"float32\")\n        x = tf.math.reduce_sum( x * broad_mask, axis=1)\n        x = x / tf.math.maximum(tf.reduce_sum(broad_mask, axis=1), tf.constant([1e-9]))\n        return x \n","metadata":{"execution":{"iopub.status.busy":"2022-11-29T02:13:18.341858Z","iopub.execute_input":"2022-11-29T02:13:18.342218Z","iopub.status.idle":"2022-11-29T02:13:18.348848Z","shell.execute_reply.started":"2022-11-29T02:13:18.342186Z","shell.execute_reply":"2022-11-29T02:13:18.347728Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"config = BertConfig.from_pretrained(\"/kaggle/input/bertbasecased/\") # need to change for kaggle\nconfig.attention_probs_dropout_prob = 0.0\nconfig.hidden_dropout_prob = 0.0\nconfig","metadata":{"execution":{"iopub.status.busy":"2022-11-29T02:13:19.868371Z","iopub.execute_input":"2022-11-29T02:13:19.868726Z","iopub.status.idle":"2022-11-29T02:13:19.882136Z","shell.execute_reply.started":"2022-11-29T02:13:19.868686Z","shell.execute_reply":"2022-11-29T02:13:19.881162Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}"},"metadata":{}}]},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [layers.Dense(dense_dim, activation='relu'),\n            layers.Dense(embed_dim),]\n        )\n\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n    \n    def call(self, inputs, mask=None):\n        if mask is not None:\n            mask = mask[:, tf.newaxis, :]\n        attention_output = self.attention(\n            inputs, inputs, attention_mask=mask\n        )\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"num_heads\": self.num_heads,\n            \"dense_dim\": self.dense_dim\n        })\n        return config\n\n\ntf.keras.backend.clear_session()\ndef build_model():\n    # Multi inputs\n    tokens = keras.Input(shape=(None,), dtype=\"int32\", name=\"tokens\")\n    attention_mask = keras.Input(shape=(None,), dtype=\"int32\", name=\"attention_mask\")\n    \n    base_model = TFBertModel.from_pretrained(\"/kaggle/input/bertbasecased/\",\n                                             from_pt=True,\n                                             config=config) # no dropout, maybe change this. \n\n    base_model.trainable = False  # Freeze bert model\n    \n    base_outputs = base_model.bert({\"input_ids\": tokens,\n                              \"attention_mask\": attention_mask})\n\n    last_hidden_state = base_outputs[0]\n    \n#     We will add Mean Pool instead of simply add a GlobalAveragePooling1D layer  \n#     x = layers.GlobalAveragePooling1D()(base_outputs[0]) \n    x_bert = MeanPool()(last_hidden_state, mask=attention_mask)\n    \n\n    embedding_layer = layers.Embedding(\n            max_tokens,\n            embed_dim,\n            embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n            trainable=False,\n            mask_zero=True\n        )\n        \n\n    tokens_full = keras.Input(shape=(None,), dtype=\"int64\", name='tokens_full')\n\n    embedded = embedding_layer(tokens_full)\n    x_transformer =  TransformerEncoder(embed_dim, dense_dim, num_heads)(embedded)\n    x_transformer = layers.GlobalAveragePooling1D()(x_transformer)\n    x_transformer = layers.Dropout(0.5)(x_transformer)\n\n\n    x = tf.keras.layers.Concatenate(axis=1)([x_bert, x_transformer])\n\n    # Add three dense layers to do regression\n    x = layers.Dense(768, activation='relu')(x)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.Dense(256, activation=\"relu\")(x)\n\n    outputs = layers.Dense(6)(x)\n    model = keras.Model([tokens, attention_mask, tokens_full] , outputs=outputs)\n   \n    model.compile(loss=keras.losses.MeanSquaredError(),\n             optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            metrics=keras.metrics.RootMeanSquaredError())\n    \n    return model\n\nmodel = build_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-11-29T02:13:36.933799Z","iopub.execute_input":"2022-11-29T02:13:36.934138Z","iopub.status.idle":"2022-11-29T02:13:48.996546Z","shell.execute_reply.started":"2022-11-29T02:13:36.934107Z","shell.execute_reply":"2022-11-29T02:13:48.995486Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ntokens_full (InputLayer)        [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, None, 300)    6000000     tokens_full[0][0]                \n__________________________________________________________________________________________________\nattention_mask (InputLayer)     [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntokens (InputLayer)             [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntransformer_encoder (Transforme (None, None, 300)    742832      embedding[0][0]                  \n__________________________________________________________________________________________________\nbert (TFBertMainLayer)          TFBaseModelOutputWit 108310272   attention_mask[0][0]             \n                                                                 tokens[0][0]                     \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 300)          0           transformer_encoder[0][0]        \n__________________________________________________________________________________________________\nmean_pool (MeanPool)            (None, 768)          0           bert[0][0]                       \n                                                                 attention_mask[0][0]             \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 300)          0           global_average_pooling1d[0][0]   \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 1068)         0           mean_pool[0][0]                  \n                                                                 dropout_37[0][0]                 \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 768)          820992      concatenate[0][0]                \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 512)          393728      dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 256)          131328      dense_3[0][0]                    \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 6)            1542        dense_4[0][0]                    \n==================================================================================================\nTotal params: 116,400,694\nTrainable params: 2,090,422\nNon-trainable params: 114,310,272\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(\n            x={\n                'tokens': train_input_ids,\n                'attention_mask': train_attention_mask,\n                'tokens_full': train_full_x\n            },\n            y =train_y,\n\n            validation_split=None,\n            epochs=EPOCHS,\n            batch_size=6,\n            )\n","metadata":{"execution":{"iopub.status.busy":"2022-11-29T02:14:04.916602Z","iopub.execute_input":"2022-11-29T02:14:04.917218Z","iopub.status.idle":"2022-11-29T02:42:35.225359Z","shell.execute_reply.started":"2022-11-29T02:14:04.917174Z","shell.execute_reply":"2022-11-29T02:42:35.224448Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"2022-11-29 02:14:16.839326: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"652/652 [==============================] - 351s 520ms/step - loss: 0.3990 - root_mean_squared_error: 0.6317\nEpoch 2/5\n652/652 [==============================] - 340s 521ms/step - loss: 0.3004 - root_mean_squared_error: 0.5480\nEpoch 3/5\n652/652 [==============================] - 340s 521ms/step - loss: 0.2823 - root_mean_squared_error: 0.5313\nEpoch 4/5\n652/652 [==============================] - 340s 521ms/step - loss: 0.2577 - root_mean_squared_error: 0.5077\nEpoch 5/5\n652/652 [==============================] - 340s 522ms/step - loss: 0.2603 - root_mean_squared_error: 0.5102\n","output_type":"stream"}]},{"cell_type":"code","source":"test_predictions = model.predict(x={\n                'tokens': test_input_ids,\n                'attention_mask': test_attention_mask,\n                'tokens_full': test_full_x\n            })\n\ntest_predictions = pd.DataFrame(test_predictions)\ntest['cohesion'] = test_predictions[0]\ntest['syntax'] = test_predictions[1]\ntest['vocabulary'] = test_predictions[2]\ntest['phraseology'] = test_predictions[3]\ntest['grammar'] = test_predictions[4]\ntest['conventions'] = test_predictions[5]\ntest","metadata":{"execution":{"iopub.status.busy":"2022-11-29T02:43:31.447554Z","iopub.execute_input":"2022-11-29T02:43:31.447923Z","iopub.status.idle":"2022-11-29T02:43:31.953234Z","shell.execute_reply.started":"2022-11-29T02:43:31.447891Z","shell.execute_reply":"2022-11-29T02:43:31.952237Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"        text_id                                          full_text  cohesion  \\\n0  0000C359D63E  when a person has no experience on a job their...  2.883920   \n1  000BAD50D026  Do you think students would benefit from being...  2.686886   \n2  00367BB2546B  Thomas Jefferson once states that \"it is wonde...  3.368145   \n\n     syntax  vocabulary  phraseology   grammar  conventions  \n0  2.803073    2.953128     2.889484  2.650794     2.689518  \n1  2.557787    2.665262     2.489653  2.239902     2.637219  \n2  3.288126    3.355875     3.325210  3.083086     3.307231  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000C359D63E</td>\n      <td>when a person has no experience on a job their...</td>\n      <td>2.883920</td>\n      <td>2.803073</td>\n      <td>2.953128</td>\n      <td>2.889484</td>\n      <td>2.650794</td>\n      <td>2.689518</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000BAD50D026</td>\n      <td>Do you think students would benefit from being...</td>\n      <td>2.686886</td>\n      <td>2.557787</td>\n      <td>2.665262</td>\n      <td>2.489653</td>\n      <td>2.239902</td>\n      <td>2.637219</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00367BB2546B</td>\n      <td>Thomas Jefferson once states that \"it is wonde...</td>\n      <td>3.368145</td>\n      <td>3.288126</td>\n      <td>3.355875</td>\n      <td>3.325210</td>\n      <td>3.083086</td>\n      <td>3.307231</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"res = test.drop(\"full_text\",axis=1)\nres.to_csv(\"/kaggle/working/submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-29T02:43:49.310536Z","iopub.execute_input":"2022-11-29T02:43:49.311221Z","iopub.status.idle":"2022-11-29T02:43:49.322594Z","shell.execute_reply.started":"2022-11-29T02:43:49.311184Z","shell.execute_reply":"2022-11-29T02:43:49.321657Z"},"trusted":true},"execution_count":24,"outputs":[]}]}