{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-04T22:17:43.775418Z",
     "iopub.status.busy": "2022-12-04T22:17:43.774736Z",
     "iopub.status.idle": "2022-12-04T22:17:43.793511Z",
     "shell.execute_reply": "2022-12-04T22:17:43.792280Z",
     "shell.execute_reply.started": "2022-12-04T22:17:43.775382Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahhoover/miniforge3/envs/w207_final/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# used this tutorial to help. \n",
    "# https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert\n",
    "# https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "# https://www.tensorflow.org/tfmodels/nlp/fine_tune_bert#import_libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer, AutoModelForSequenceClassification, TFBertModel, BertConfig\n",
    "\n",
    "from tqdm import tqdm\n",
    "# plots and images\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "#sklearn processing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from textblob import Word\n",
    "\n",
    "# import wandb\n",
    "\n",
    "\n",
    "# #configs\n",
    "# max_tokens = 20000\n",
    "# embed_dim  = 300\n",
    "# num_heads  = 2\n",
    "# dense_dim  = 32\n",
    "\n",
    "\n",
    "developing = True\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 3200\n",
    "SEQ_LEN = 1536\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T22:17:46.744241Z",
     "iopub.status.busy": "2022-12-04T22:17:46.743868Z",
     "iopub.status.idle": "2022-12-04T22:17:46.963395Z",
     "shell.execute_reply": "2022-12-04T22:17:46.962447Z",
     "shell.execute_reply.started": "2022-12-04T22:17:46.744209Z"
    }
   },
   "outputs": [],
   "source": [
    "#sample_submission_raw = pd.read_csv('/Users/sarahhoover/Desktop/W207/kaggle_english_language_grading/sample_submission.csv')\n",
    "train_data_raw = pd.read_csv('/Users/sarahhoover/Desktop/W207/kaggle_english_language_grading/train.csv')\n",
    "test_from_comp = pd.read_csv('/Users/sarahhoover/Desktop/W207/kaggle_english_language_grading/test.csv')\n",
    "\n",
    "\n",
    "train, test = train_test_split(train_data_raw, test_size=0.2, random_state=21) #20% for test\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=21) # 10% for validation\n",
    "\n",
    "\n",
    "targets=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "\n",
    "train_y = train[targets]\n",
    "val_y = val[targets]\n",
    "test_y = test[targets]\n",
    "\n",
    "train_x = train['full_text']\n",
    "val_x = val['full_text']\n",
    "test_x = test['full_text']\n",
    "test_data_comp = test_from_comp['full_text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T22:17:55.048570Z",
     "iopub.status.busy": "2022-12-04T22:17:55.048224Z",
     "iopub.status.idle": "2022-12-04T22:18:33.018128Z",
     "shell.execute_reply": "2022-12-04T22:18:33.017153Z",
     "shell.execute_reply.started": "2022-12-04T22:17:55.048540Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 2815/2815 [00:09<00:00, 297.45it/s]\n",
      "100%|██████████████████████████████████████████| 313/313 [00:01<00:00, 301.99it/s]\n",
      "100%|██████████████████████████████████████████| 783/783 [00:02<00:00, 306.14it/s]\n",
      "100%|██████████████████████████████████████████████| 3/3 [00:00<00:00, 243.43it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/Users/sarahhoover/Desktop/W207/kaggle_english_language_grading/bertbasecased')\n",
    "\n",
    "def get_ids_mask(inputs):\n",
    "    input_ids_part_one = []\n",
    "    attention_mask_part_one = []\n",
    "    \n",
    "    input_ids_part_two = []\n",
    "    attention_mask_part_two = []\n",
    "    \n",
    "    input_ids_part_three = []\n",
    "    attention_mask_part_three = []\n",
    "    \n",
    "\n",
    "    for x in tqdm(inputs):\n",
    "        tokens = tokenizer(x, padding=\"max_length\", truncation=True, max_length=SEQ_LEN, return_tensors=\"np\")\n",
    "        ids = tokens[\"input_ids\"]\n",
    "        mask = tokens[\"attention_mask\"]\n",
    "        \n",
    "        input_ids_part_one.append(ids[0][:512])\n",
    "        attention_mask_part_one.append(mask[0][:512])\n",
    "        \n",
    "        input_ids_part_two.append(ids[0][512:1024])\n",
    "        attention_mask_part_two.append(mask[0][512:1024])\n",
    "        \n",
    "        input_ids_part_three.append(ids[0][1024:1536])\n",
    "        attention_mask_part_three.append(mask[0][1024:1536])\n",
    "        \n",
    "    input_ids_part_one = np.array(input_ids_part_one).squeeze()\n",
    "    attention_mask_part_one = np.array(attention_mask_part_one).squeeze()\n",
    "    \n",
    "    input_ids_part_two = np.array(input_ids_part_two).squeeze()\n",
    "    attention_mask_part_two = np.array(attention_mask_part_two).squeeze()\n",
    "    \n",
    "    input_ids_part_three = np.array(input_ids_part_three).squeeze()\n",
    "    attention_mask_part_three = np.array(attention_mask_part_three).squeeze()\n",
    "    \n",
    "    return input_ids_part_one, attention_mask_part_one, input_ids_part_two, attention_mask_part_two, input_ids_part_three, attention_mask_part_three\n",
    "\n",
    "\n",
    "train_input_ids_part_one, train_attention_mask_part_one, train_input_ids_part_two, train_attention_mask_part_two, train_input_ids_part_three, train_attention_mask_part_three = get_ids_mask(train_x)\n",
    "val_input_ids_part_one, val_attention_mask_part_one, val_input_ids_part_two, val_attention_mask_part_two, val_input_ids_part_three, val_attention_mask_part_three = get_ids_mask(val_x)\n",
    "test_input_ids_part_one, test_attention_mask_part_one, test_input_ids_part_two, test_attention_mask_part_two, test_input_ids_part_three, test_attention_mask_part_three = get_ids_mask(test_x)\n",
    "test_data_comp_input_ids_part_one, test_data_comp_attention_mask_part_one, test_data_comp_input_ids_part_two, test_data_comp_attention_mask_part_two, test_data_comp_input_ids_part_three, test_data_comp_attention_mask_part_three = get_ids_mask(test_data_comp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T22:19:59.899089Z",
     "iopub.status.busy": "2022-12-04T22:19:59.898742Z",
     "iopub.status.idle": "2022-12-04T22:19:59.905744Z",
     "shell.execute_reply": "2022-12-04T22:19:59.904770Z",
     "shell.execute_reply.started": "2022-12-04T22:19:59.899061Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanPool(keras.layers.Layer):\n",
    "    def call(self, x, mask=None):\n",
    "        broad_mask = tf.cast(tf.expand_dims(mask, -1), \"float32\")\n",
    "        x = tf.math.reduce_sum( x * broad_mask, axis=1)\n",
    "        x = x / tf.math.maximum(tf.reduce_sum(broad_mask, axis=1), tf.constant([1e-9]))\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T22:21:05.578272Z",
     "iopub.status.busy": "2022-12-04T22:21:05.577894Z",
     "iopub.status.idle": "2022-12-04T22:21:20.699589Z",
     "shell.execute_reply": "2022-12-04T22:21:20.698503Z",
     "shell.execute_reply.started": "2022-12-04T22:21:05.578241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"/Users/sarahhoover/Desktop/W207/kaggle_english_language_grading/bert_transfer_model.keras\",                 \n",
    "   custom_objects={\n",
    "        \"MeanPool\": MeanPool\n",
    "\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T22:22:52.717990Z",
     "iopub.status.busy": "2022-12-04T22:22:52.717604Z",
     "iopub.status.idle": "2022-12-04T22:23:00.777964Z",
     "shell.execute_reply": "2022-12-04T22:23:00.777032Z",
     "shell.execute_reply.started": "2022-12-04T22:22:52.717949Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 22:04:08.822739: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>cohesion_predicted</th>\n",
       "      <th>syntax_predicted</th>\n",
       "      <th>vocabulary_predicted</th>\n",
       "      <th>phraseology_predicted</th>\n",
       "      <th>grammar_predicted</th>\n",
       "      <th>conventions_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>457</td>\n",
       "      <td>2286BCFF86EB</td>\n",
       "      <td>Generic_Name was sitting in the cafeteria list...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.048340</td>\n",
       "      <td>4.130281</td>\n",
       "      <td>4.187365</td>\n",
       "      <td>4.127691</td>\n",
       "      <td>3.923795</td>\n",
       "      <td>4.048772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2863</td>\n",
       "      <td>CFD454F55693</td>\n",
       "      <td>The American jazz legend Duke Ellington was fa...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.665304</td>\n",
       "      <td>2.528556</td>\n",
       "      <td>2.916800</td>\n",
       "      <td>2.696450</td>\n",
       "      <td>2.600939</td>\n",
       "      <td>2.407343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3299</td>\n",
       "      <td>E4F813D54870</td>\n",
       "      <td>In some societies, students are expected to id...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.558933</td>\n",
       "      <td>3.654609</td>\n",
       "      <td>3.776189</td>\n",
       "      <td>3.687513</td>\n",
       "      <td>3.698242</td>\n",
       "      <td>3.785335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2397</td>\n",
       "      <td>B2123357B114</td>\n",
       "      <td>People have said \"positive attitude is the key...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.167751</td>\n",
       "      <td>3.106399</td>\n",
       "      <td>3.251005</td>\n",
       "      <td>3.125591</td>\n",
       "      <td>3.079607</td>\n",
       "      <td>3.158029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3797</td>\n",
       "      <td>FAF2897C9B65</td>\n",
       "      <td>I agree with Winston Churchill that success co...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.185337</td>\n",
       "      <td>3.064423</td>\n",
       "      <td>3.276067</td>\n",
       "      <td>3.202967</td>\n",
       "      <td>3.167386</td>\n",
       "      <td>3.086954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>672</td>\n",
       "      <td>340BCFE7E249</td>\n",
       "      <td>When elementry students have a lesson on what ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.019997</td>\n",
       "      <td>3.043649</td>\n",
       "      <td>3.063476</td>\n",
       "      <td>2.951304</td>\n",
       "      <td>2.788896</td>\n",
       "      <td>3.072680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>2195</td>\n",
       "      <td>A3A5003C8AD1</td>\n",
       "      <td>Something that i would like to accomplish is r...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.209097</td>\n",
       "      <td>3.451641</td>\n",
       "      <td>3.625768</td>\n",
       "      <td>3.502521</td>\n",
       "      <td>3.535727</td>\n",
       "      <td>3.399922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>2876</td>\n",
       "      <td>D06EDF336392</td>\n",
       "      <td>A city council is debating for the adoption of...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.417498</td>\n",
       "      <td>3.454386</td>\n",
       "      <td>3.652242</td>\n",
       "      <td>3.409591</td>\n",
       "      <td>3.153137</td>\n",
       "      <td>3.324554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>2117</td>\n",
       "      <td>9EADCECA2FB8</td>\n",
       "      <td>i think that students should have cell phones ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.805951</td>\n",
       "      <td>2.816587</td>\n",
       "      <td>3.005169</td>\n",
       "      <td>2.840981</td>\n",
       "      <td>2.916574</td>\n",
       "      <td>2.885063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>295</td>\n",
       "      <td>15BDA0989B56</td>\n",
       "      <td>When you trying to make something different ev...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.510152</td>\n",
       "      <td>2.573400</td>\n",
       "      <td>3.036493</td>\n",
       "      <td>2.476513</td>\n",
       "      <td>2.289907</td>\n",
       "      <td>2.416072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>783 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       text_id                                          full_text  \\\n",
       "0      457  2286BCFF86EB  Generic_Name was sitting in the cafeteria list...   \n",
       "1     2863  CFD454F55693  The American jazz legend Duke Ellington was fa...   \n",
       "2     3299  E4F813D54870  In some societies, students are expected to id...   \n",
       "3     2397  B2123357B114  People have said \"positive attitude is the key...   \n",
       "4     3797  FAF2897C9B65  I agree with Winston Churchill that success co...   \n",
       "..     ...           ...                                                ...   \n",
       "778    672  340BCFE7E249  When elementry students have a lesson on what ...   \n",
       "779   2195  A3A5003C8AD1  Something that i would like to accomplish is r...   \n",
       "780   2876  D06EDF336392  A city council is debating for the adoption of...   \n",
       "781   2117  9EADCECA2FB8  i think that students should have cell phones ...   \n",
       "782    295  15BDA0989B56  When you trying to make something different ev...   \n",
       "\n",
       "     cohesion  syntax  vocabulary  phraseology  grammar  conventions  \\\n",
       "0         4.0     4.0         3.5          4.0      4.0          3.5   \n",
       "1         2.5     1.5         3.0          3.0      2.5          3.0   \n",
       "2         3.5     3.0         3.0          3.0      3.0          3.0   \n",
       "3         3.5     3.0         3.0          3.0      2.5          2.5   \n",
       "4         3.5     3.5         4.0          3.0      3.5          4.0   \n",
       "..        ...     ...         ...          ...      ...          ...   \n",
       "778       2.5     3.0         3.5          3.0      3.5          3.5   \n",
       "779       3.5     4.0         3.5          3.5      4.0          4.0   \n",
       "780       3.0     3.0         2.5          2.5      2.5          3.0   \n",
       "781       2.5     3.0         3.0          2.5      3.0          3.0   \n",
       "782       2.0     2.5         2.5          3.0      2.0          2.0   \n",
       "\n",
       "     cohesion_predicted  syntax_predicted  vocabulary_predicted  \\\n",
       "0              4.048340          4.130281              4.187365   \n",
       "1              2.665304          2.528556              2.916800   \n",
       "2              3.558933          3.654609              3.776189   \n",
       "3              3.167751          3.106399              3.251005   \n",
       "4              3.185337          3.064423              3.276067   \n",
       "..                  ...               ...                   ...   \n",
       "778            3.019997          3.043649              3.063476   \n",
       "779            3.209097          3.451641              3.625768   \n",
       "780            3.417498          3.454386              3.652242   \n",
       "781            2.805951          2.816587              3.005169   \n",
       "782            2.510152          2.573400              3.036493   \n",
       "\n",
       "     phraseology_predicted  grammar_predicted  conventions_predicted  \n",
       "0                 4.127691           3.923795               4.048772  \n",
       "1                 2.696450           2.600939               2.407343  \n",
       "2                 3.687513           3.698242               3.785335  \n",
       "3                 3.125591           3.079607               3.158029  \n",
       "4                 3.202967           3.167386               3.086954  \n",
       "..                     ...                ...                    ...  \n",
       "778               2.951304           2.788896               3.072680  \n",
       "779               3.502521           3.535727               3.399922  \n",
       "780               3.409591           3.153137               3.324554  \n",
       "781               2.840981           2.916574               2.885063  \n",
       "782               2.476513           2.289907               2.416072  \n",
       "\n",
       "[783 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = model.predict(x={\n",
    "            'tokens_one': test_input_ids_part_one,\n",
    "            'attention_mask_one': test_attention_mask_part_one,\n",
    "            \n",
    "            'tokens_two': test_input_ids_part_two,\n",
    "            'attention_mask_two': test_attention_mask_part_two,\n",
    "            \n",
    "            'tokens_three': test_input_ids_part_three,\n",
    "            'attention_mask_three': test_attention_mask_part_three,\n",
    "        })\n",
    "\n",
    "\n",
    "test_predictions = pd.DataFrame(test_predictions)\n",
    "test = test.reset_index()\n",
    "test['cohesion_predicted'] = test_predictions[0]\n",
    "test['syntax_predicted'] = test_predictions[1]\n",
    "test['vocabulary_predicted'] = test_predictions[2]\n",
    "test['phraseology_predicted'] = test_predictions[3]\n",
    "test['grammar_predicted'] = test_predictions[4]\n",
    "test['conventions_predicted'] = test_predictions[5]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(x={\n",
    "            'tokens_one': train_input_ids_part_one,\n",
    "            'attention_mask_one': train_attention_mask_part_one,\n",
    "            \n",
    "            'tokens_two': train_input_ids_part_two,\n",
    "            'attention_mask_two': train_attention_mask_part_two,\n",
    "            \n",
    "            'tokens_three': train_input_ids_part_three,\n",
    "            'attention_mask_three': train_attention_mask_part_three,\n",
    "        })\n",
    "\n",
    "\n",
    "train_predictions = pd.DataFrame(train_predictions)\n",
    "train = train.reset_index()\n",
    "train['cohesion_predicted'] = train_predictions[0]\n",
    "train['syntax_predicted'] = train_predictions[1]\n",
    "train['vocabulary_predicted'] = train_predictions[2]\n",
    "train['phraseology_predicted'] = train_predictions[3]\n",
    "train['grammar_predicted'] = train_predictions[4]\n",
    "train['conventions_predicted'] = train_predictions[5]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = model.predict(x={\n",
    "            'tokens_one': val_input_ids_part_one,\n",
    "            'attention_mask_one': val_attention_mask_part_one,\n",
    "            \n",
    "            'tokens_two': val_input_ids_part_two,\n",
    "            'attention_mask_two': val_attention_mask_part_two,\n",
    "            \n",
    "            'tokens_three': val_input_ids_part_three,\n",
    "            'attention_mask_three': val_attention_mask_part_three,\n",
    "        })\n",
    "\n",
    "\n",
    "val_predictions = pd.DataFrame(val_predictions)\n",
    "val = val.reset_index()\n",
    "val['cohesion_predicted'] = val_predictions[0]\n",
    "val['syntax_predicted'] = val_predictions[1]\n",
    "val['vocabulary_predicted'] = val_predictions[2]\n",
    "val['phraseology_predicted'] = val_predictions[3]\n",
    "val['grammar_predicted'] = val_predictions[4]\n",
    "val['conventions_predicted'] = val_predictions[5]\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features for subgroup analysis\n",
    "for df in [test, train, val]:\n",
    "\n",
    "    # word counts\n",
    "    df['word_count'] = df['full_text'].map(lambda x:len(x.split()))\n",
    "\n",
    "    # average word length\n",
    "    df['avg_word_len'] = df['full_text'].map(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "    # sentence count\n",
    "    df['sent_count'] = df['full_text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "\n",
    "    # average sentence length\n",
    "    df['avg_sent_len'] = df['full_text'].apply(lambda x: np.mean([len(w.split()) for w in sent_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-04T22:23:15.766129Z",
     "iopub.status.busy": "2022-12-04T22:23:15.765744Z",
     "iopub.status.idle": "2022-12-04T22:23:15.776862Z",
     "shell.execute_reply": "2022-12-04T22:23:15.775882Z",
     "shell.execute_reply.started": "2022-12-04T22:23:15.766083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create subgroups based on engineered features\n",
    "\n",
    "# Define a function to create groups based on quantiles\n",
    "def create_percentiles(df, feature):\n",
    "    \n",
    "    # determine 25, 50, and 75 percentiles for the feature\n",
    "    perc_25 = df[feature].quantile(0.25)\n",
    "    perc_50 = df[feature].quantile(0.5)\n",
    "    perc_75 = df[feature].quantile(0.75)\n",
    "    \n",
    "    # create booleans to flag if an observation is at or below the percentile\n",
    "    df['less_25'] = (df[feature] <= perc_25).astype(int)\n",
    "    df['less_50'] = (df[feature] <= perc_50).astype(int)\n",
    "    df['less_75'] = (df[feature] <= perc_75).astype(int)\n",
    "    \n",
    "    # Sum the booleans (this helps us determine which category the observation falls into)\n",
    "    df['sum'] = df['less_25'] + df['less_50'] + df['less_75']\n",
    "    \n",
    "    # Assign category labels and create categorical variable\n",
    "    dict = {0: '75th-100th', 1: '50th-75th', 2: '25th-50th', 3: '0th-25th%'}\n",
    "    df[feature+'_subgroup'] = df['sum'].apply(lambda x: dict[x])\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns = ['less_25', 'less_50', 'less_75', 'sum'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the subgroups across all datasets, for all engineered features\n",
    "for df in [test, train, val]:\n",
    "    for feature in ['word_count', 'avg_word_len', 'sent_count', 'avg_sent_len']:\n",
    "        create_percentiles(df, feature)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE across each subgroup\n",
    "\n",
    "# Write a function to calculate the SE for a given feature\n",
    "from math import sqrt\n",
    "def calc_se(df, outcome):\n",
    "    df[outcome + '_se'] = (df[outcome]-df[outcome+'_predicted'])**2\n",
    "    return df\n",
    "\n",
    "# Apply the SE function for each outcome in each dataset\n",
    "for df in [test, train, val]:\n",
    "    for outcome in ['cohesion', 'grammar', 'vocabulary', 'phraseology', 'conventions']:\n",
    "        calc_se(df, outcome)\n",
    "\n",
    "# Create a dictionary with the aggregation function to use for each SE: this is the RMSE\n",
    "agg_dict = {outcome+'_se': (lambda x: sqrt(np.mean(x))) for outcome in ['cohesion', 'grammar', 'vocabulary', 'phraseology', 'conventions']}\n",
    "\n",
    "# Calculate RMSE for each feature for each subgroup.\n",
    "# Then, calculate column-wise average of the RMSEs across all outcomes. This is our primary metric we are breaking out by subgroup\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "fig.tight_layout()\n",
    "i = 1\n",
    "for df in [test, train, val]:\n",
    "    for feature in ['word_count', 'avg_word_len', 'sent_count', 'avg_sent_len']:\n",
    "        subgroup = df.groupby(feature+'_subgroup').agg(agg_dict).reset_index()\n",
    "        subgroup['RMSE'] = subgroup.mean(axis = 1)\n",
    "        subgroup = subgroup[[feature+'_subgroup','RMSE']]\n",
    "        plt.subplot(2, 2, i)\n",
    "        plt.bar(subgroup[feature+'_subgroup'], subgroup['RMSE'])\n",
    "        plt.subplots_adjust(hspace = 0.18)\n",
    "        plt.title(feature)\n",
    "        plt.xlabel('Percentile')\n",
    "        plt.ylabel('RMSE')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This source helped with the scatterplot code:\n",
    "# https://kanoki.org/2020/08/30/matplotlib-scatter-plot-color-by-category-in-python/\n",
    "test.assign(group = \"test\")\n",
    "train.assign(group = \"train\")\n",
    "val.assign(group = \"validation\")\n",
    "full = pd.concat([train, test, val])\n",
    "colors = {'train':'red', 'test':'green', 'validation':'yellow'}\n",
    "\n",
    "fig, axs = plt.subplots(nrows=6, ncols=4, figsize=(10,10))\n",
    "fig.tight_layout()\n",
    "i = 1\n",
    "for outcome in ['cohesion', 'grammar', 'vocabulary', 'phraseology', 'conventions']\n",
    "    for feature in ['word_count', 'avg_word_len', 'sent_count', 'avg_sent_len']:\n",
    "        plt.subplot(6, 4, i)\n",
    "        plt.scatter(full[feature], full[outcome+'_se'], c=full['group'].map(colors))\n",
    "        plt.subplots_adjust(hspace = 0.18)\n",
    "        plt.title(feature)\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel(outcome+ ' SE')\n",
    "        i += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w207",
   "language": "python",
   "name": "w207"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}